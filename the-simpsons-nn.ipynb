{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:00.799473500Z",
     "start_time": "2023-09-29T21:58:00.779319500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# device = \"cpu\" # uncomment if you want to use \"cpu\", currently cpu is faster than cuda (maybe because the NN is very little)\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:01.326448500Z",
     "start_time": "2023-09-29T21:58:01.295448300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a custom Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class SimpsonsImageDataset(Dataset):\n",
    "    def __init__(self, tensor, label):\n",
    "        self.tensor = tensor\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensor[index], self.label[index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:02.615695600Z",
     "start_time": "2023-09-29T21:58:02.603663Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading an image and creating a label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "all_labels = [\"abraham_grampa_simpson\",\n",
    "              \"agnes_skinner\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:06.334827800Z",
     "start_time": "2023-09-29T21:58:06.321841500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Functions for image handling\n",
    "\n",
    "def show_image_by_path(_image_path: str) -> None:\n",
    "    image = Image.open(_image_path)\n",
    "    image.show()\n",
    "\n",
    "def image_to_tensor(_image_path: str) -> torch.Tensor:\n",
    "    image = Image.open(_image_path)\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.RandomRotation(15),\n",
    "                                    transforms.ToTensor()])\n",
    "    _image_tensor = transform(image)  # image_tensor now has a shape of torch.Size([3, 224, 224])\n",
    "\n",
    "    # RandomHorizontalFlip\n",
    "    # --> randomly mirror the image from the left to right\n",
    "    # RandomRotation\n",
    "    # --> rotate the image by a random angle within a given range, in this case from -15 to +15 degrees\n",
    "\n",
    "    # we add a batch dimension since most neural network frameworks expect input in the form of batches\n",
    "    # the batch dimension helps in parallel processing and is essential for training the model with\n",
    "    # multiple samples\n",
    "    _image_tensor = _image_tensor.unsqueeze(0)  # image_tensor now has a shape of torch.Size([1, 3, 224, 224])\n",
    "\n",
    "    # image_tensor now has these dimensions: [batch_size, channels, height, width]\n",
    "    # the channel dimension refers to the different color layers that make up an image. Usually, we have 3 channels: RGB\n",
    "    # by using transforms.ToTensor(), we automatically normalize the pixel values to a range between 0 and 1 (instead of 0 to 255).\n",
    "    # it is important to understand each value in the multidimensional array is between 0 and 1 now\n",
    "\n",
    "    return _image_tensor\n",
    "\n",
    "def show_image_by_tensor(_image_tensor: torch.Tensor) -> None:\n",
    "    _image_tensor = _image_tensor.squeeze(0)  # remove the batch dimension\n",
    "    transform = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "    # convert tensor to PIL image\n",
    "    image_pil = transform(_image_tensor)\n",
    "\n",
    "    # display the image\n",
    "    image_pil.show()\n",
    "\n",
    "\n",
    "def get_label_for_image_path(_image_path: str) -> torch.Tensor:\n",
    "    # here we are returning a tensor with just one dimension - it is equal to the size of the batch dimension of a single image\n",
    "    # depending on the image_path, a label tensor with value between 0 and 19 is created (since we have 20 different characters)\n",
    "    directory = os.path.basename(os.path.dirname(_image_path))\n",
    "    label_idx = all_labels.index(directory)\n",
    "    return torch.tensor([label_idx], dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:06.750679800Z",
     "start_time": "2023-09-29T21:58:06.729679600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "image_path_grampa = \"data/train/abraham_grampa_simpson/pic_0000.jpg\"\n",
    "image_path_agnes = \"data/train/agnes_skinner/pic_0000.jpg\"\n",
    "show_image_by_path(image_path_grampa)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:31.223024Z",
     "start_time": "2023-09-29T21:58:27.776521Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "image_tensor_grampa = image_to_tensor(image_path_grampa)\n",
    "image_tensor_agnes = image_to_tensor(image_path_agnes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:32.580918600Z",
     "start_time": "2023-09-29T21:58:32.558919500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "label_tensor_grampa = get_label_for_image_path(image_path_grampa)\n",
    "label_tensor_agnes = get_label_for_image_path(image_path_agnes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:32.930765200Z",
     "start_time": "2023-09-29T21:58:32.919736700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "show_image_by_tensor(image_tensor_agnes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:44.349768400Z",
     "start_time": "2023-09-29T21:58:40.936596100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "image_tensor_combined = torch.cat([image_tensor_grampa, image_tensor_agnes], dim=0)\n",
    "label_tensor_combined = torch.cat([label_tensor_grampa, label_tensor_agnes], dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:45.675996400Z",
     "start_time": "2023-09-29T21:58:45.666997800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "simpsons_dataset = SimpsonsImageDataset(image_tensor_combined, label_tensor_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:46.439614200Z",
     "start_time": "2023-09-29T21:58:46.423611200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting into train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "total_size = len(simpsons_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(simpsons_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T21:58:47.540370900Z",
     "start_time": "2023-09-29T21:58:47.519380Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
