{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wandb login API-KEY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import wandb\n",
    "from shared_methods import all_labels\n",
    "from simpsons_neural_network_2 import SimpsonsNet2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:39:05.301219300Z",
     "start_time": "2023-10-01T13:39:02.405583800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check if we can use Cuda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# device = \"cpu\" # uncomment if you want to use \"cpu\", currently cpu is faster than cuda (maybe because the NN is very little)\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:39:10.357516300Z",
     "start_time": "2023-10-01T13:39:10.318489800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config = argparse.Namespace()\n",
    "config.learning_rate = 0.01\n",
    "config.epochs = 30\n",
    "config.batch_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:39:12.055420700Z",
     "start_time": "2023-10-01T13:39:12.045390600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a custom Dataset Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SimpsonsImageDataset(Dataset):\n",
    "    def __init__(self, tensor, label):\n",
    "        self.tensor = tensor\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensor[index], self.label[index]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:39:14.654499600Z",
     "start_time": "2023-10-01T13:39:14.644534300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading an image and creating a label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Functions for image handling\n",
    "\n",
    "def image_to_tensor(_image_path: str) -> torch.Tensor:\n",
    "    image = Image.open(_image_path)\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor()])\n",
    "    _image_tensor = transform(image)  # image_tensor now has a shape of torch.Size([3, 224, 224])\n",
    "\n",
    "    # RandomHorizontalFlip\n",
    "    # --> randomly mirror the image from the left to right\n",
    "    # RandomRotation\n",
    "    # --> rotate the image by a random angle within a given range, in this case from -15 to +15 degrees\n",
    "    # when we omitted those steps, we received better results\n",
    "    # compare:\n",
    "    # https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/1c2ehxl7?workspace=user-hamm-daniel (preprocessing)\n",
    "    # https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/cxjx9df9/workspace?workspace=user-hamm-daniel (no preprocessing)\n",
    "\n",
    "    # we add a batch dimension since most neural network frameworks expect input in the form of batches\n",
    "    # the batch dimension helps in parallel processing and is essential for training the model with\n",
    "    # multiple samples\n",
    "    # _image_tensor = _image_tensor.unsqueeze(0)  # image_tensor now has a shape of torch.Size([1, 3, 224, 224])\n",
    "    # --> this is not necessary if we use a Dataloader, which adds the batch dimension automatically\n",
    "\n",
    "    # image_tensor now has these dimensions: [batch_size, channels, height, width]\n",
    "    # the channel dimension refers to the different color layers that make up an image. Usually, we have 3 channels: RGB\n",
    "    # by using transforms.ToTensor(), we automatically normalize the pixel values to a range between 0 and 1 (instead of 0 to 255).\n",
    "    # it is important to understand each value in the multidimensional array is between 0 and 1 now\n",
    "\n",
    "    return _image_tensor.to(device)\n",
    "\n",
    "def show_image_by_tensor(_image_tensor: torch.Tensor) -> None:\n",
    "    _image_tensor = _image_tensor.squeeze(0)  # remove the batch dimension\n",
    "    transform = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "    # convert tensor to PIL image\n",
    "    image_pil = transform(_image_tensor)\n",
    "\n",
    "    # display the image\n",
    "    image_pil.show()\n",
    "\n",
    "\n",
    "def get_label_for_image_path(_image_path: str) -> torch.Tensor:\n",
    "    # here we are returning a tensor with just one dimension - it is equal to the size of the batch dimension of a single image\n",
    "    # depending on the image_path, a label tensor with value between 0 and 19 is created (since we have 20 different characters)\n",
    "    directory = os.path.basename(os.path.dirname(_image_path))\n",
    "    label_idx = all_labels.index(directory)\n",
    "    return torch.tensor(label_idx, dtype=torch.long).to(device)\n",
    "\n",
    "def get_character_for_label(_label_tensor: str) -> str:\n",
    "    return all_labels[_label_tensor[0]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:39:17.814207300Z",
     "start_time": "2023-10-01T13:39:17.795213700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder data/train\\abraham_grampa_simpson\n",
      "Processing folder data/train\\agnes_skinner\n",
      "Processing folder data/train\\apu_nahasapeemapetilon\n",
      "Processing folder data/train\\barney_gumble\n",
      "Processing folder data/train\\bart_simpson\n",
      "Processing folder data/train\\carl_carlson\n",
      "Processing folder data/train\\charles_montgomery_burns\n",
      "Processing folder data/train\\chief_wiggum\n",
      "Processing folder data/train\\cletus_spuckler\n",
      "Processing folder data/train\\comic_book_guy\n",
      "Processing folder data/train\\disco_stu\n",
      "Processing folder data/train\\edna_krabappel\n",
      "Processing folder data/train\\fat_tony\n",
      "Processing folder data/train\\gil\n",
      "Processing folder data/train\\groundskeeper_willie\n",
      "Processing folder data/train\\homer_simpson\n",
      "Processing folder data/train\\kent_brockman\n",
      "Processing folder data/train\\krusty_the_clown\n",
      "Processing folder data/train\\lenny_leonard\n",
      "Processing folder data/train\\lionel_hutz\n",
      "Processing folder data/train\\lisa_simpson\n",
      "Processing folder data/train\\maggie_simpson\n",
      "Processing folder data/train\\marge_simpson\n",
      "Processing folder data/train\\martin_prince\n",
      "Processing folder data/train\\mayor_quimby\n",
      "Processing folder data/train\\milhouse_van_houten\n",
      "Processing folder data/train\\miss_hoover\n",
      "Processing folder data/train\\moe_szyslak\n",
      "Processing folder data/train\\ned_flanders\n",
      "Finished processing, got 17026 image tensors and 17026 label tensors\n"
     ]
    }
   ],
   "source": [
    "image_tensors = []\n",
    "label_tensors = []\n",
    "\n",
    "root_dir = \"data/train\"\n",
    "\n",
    "for character in os.listdir(root_dir):\n",
    "    char_dir = os.path.join(root_dir, character)\n",
    "\n",
    "    # check if it's a folder\n",
    "    if os.path.isdir(char_dir):\n",
    "        print(f\"Processing folder {char_dir}\")\n",
    "\n",
    "        # iterate through all the files\n",
    "        for filename in os.listdir(char_dir):\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(char_dir, filename)\n",
    "\n",
    "                # load the tensor and the label for the image\n",
    "                image_tensor = image_to_tensor(img_path)\n",
    "                label_tensor = get_label_for_image_path(img_path)\n",
    "\n",
    "                # append to list\n",
    "                image_tensors.append(image_tensor)\n",
    "                label_tensors.append(label_tensor)\n",
    "\n",
    "print(f\"Finished processing, got {len(image_tensors)} image tensors and {len(label_tensors)} label tensors\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:40:25.839123800Z",
     "start_time": "2023-10-01T13:39:21.512052400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "image_tensor_combined = torch.stack(image_tensors)\n",
    "label_tensor_combined = torch.stack(label_tensors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:40:31.593451500Z",
     "start_time": "2023-10-01T13:40:29.321070500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "simpsons_dataset = SimpsonsImageDataset(image_tensor_combined, label_tensor_combined)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:40:32.446226500Z",
     "start_time": "2023-10-01T13:40:32.421193600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting into train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using 13620 images for training and 3406 images for validation\n"
     ]
    }
   ],
   "source": [
    "total_size = len(simpsons_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "validation_size = total_size - train_size\n",
    "\n",
    "train_dataset, validation_dataset = random_split(simpsons_dataset, [train_size, validation_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "print(f\"I am using {len(train_dataset)} images for training and {len(validation_dataset)} images for validation\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:40:34.494007500Z",
     "start_time": "2023-10-01T13:40:34.472007300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network Architecture, loss function and optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpsonsNet2(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=25088, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "model = SimpsonsNet2()\n",
    "config.model = model.__class__\n",
    "print(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "# adding a scheduler to reduce the learning_rate as soon as the validation loss stops decreasing.\n",
    "# this is to try to prevent overfitting of the model\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')  # 'min' means reducing the LR when the metric stops decreasing\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:40:40.047426300Z",
     "start_time": "2023-10-01T13:40:39.925382100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhamm-daniel\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\dev\\projects\\kaggle-the-simpsons\\wandb\\run-20231001_154047-mz69lif4</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/mz69lif4' target=\"_blank\">fresh-planet-8</a></strong> to <a href='https://wandb.ai/hamm-daniel/kaggle-simpsons' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/hamm-daniel/kaggle-simpsons' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-simpsons</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/mz69lif4' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/mz69lif4</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/mz69lif4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x2158e3b35e0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"kaggle-simpsons\", config=vars(config))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T13:40:48.487486300Z",
     "start_time": "2023-10-01T13:40:44.539982100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 1, Train Loss: 2.1559062594539125, Validation Loss: 1.6470178287720012\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 2, Train Loss: 1.4071181562025223, Validation Loss: 1.4765519962132534\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 3, Train Loss: 1.137740563618745, Validation Loss: 1.14734801287963\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 4, Train Loss: 0.9335350497087962, Validation Loss: 0.9735752043322982\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 5, Train Loss: 0.7647644806775689, Validation Loss: 1.0369433649232453\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 6, Train Loss: 0.6291896362609707, Validation Loss: 0.8489662798765664\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 7, Train Loss: 0.5236839174419782, Validation Loss: 0.8082453928818213\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 8, Train Loss: 0.41778377957747015, Validation Loss: 0.9258371817174359\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 9, Train Loss: 0.32985067064983187, Validation Loss: 0.7197535458569214\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 10, Train Loss: 0.2586419317923801, Validation Loss: 0.6937002735995801\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 11, Train Loss: 0.19922928519748118, Validation Loss: 0.7432912645217414\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 12, Train Loss: 0.14975940654745123, Validation Loss: 0.8925963768335147\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 13, Train Loss: 0.12224370397041451, Validation Loss: 0.6559844338726775\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 14, Train Loss: 0.09138806885930364, Validation Loss: 0.6756287615432918\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 15, Train Loss: 0.07745770189447652, Validation Loss: 0.786759423039784\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 16, Train Loss: 0.0637910717905897, Validation Loss: 0.848265413856395\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 17, Train Loss: 0.05672568130577989, Validation Loss: 0.6889098116727633\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 18, Train Loss: 0.04001086873293115, Validation Loss: 0.7667794410034875\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 19, Train Loss: 0.03847866431873362, Validation Loss: 0.7132787799863057\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 20, Train Loss: 0.034592801276506956, Validation Loss: 0.7119119979232271\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 21, Train Loss: 0.030902464177783036, Validation Loss: 0.7802669059868171\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 22, Train Loss: 0.030507756852678203, Validation Loss: 0.7450580531470129\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 23, Train Loss: 0.025770607675636676, Validation Loss: 0.7197124047257076\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 24, Train Loss: 0.02327766969399374, Validation Loss: 0.7477784656057848\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 25, Train Loss: 0.01481493514513776, Validation Loss: 0.6776861652174843\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 26, Train Loss: 0.011916478996890916, Validation Loss: 0.674876164923483\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 27, Train Loss: 0.01090140685224862, Validation Loss: 0.6788793894175057\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 28, Train Loss: 0.01153877953034902, Validation Loss: 0.6798457863314129\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 29, Train Loss: 0.009250561868364184, Validation Loss: 0.6914424871521019\n",
      "batch 0 from 426 ...\n",
      "batch 50 from 426 ...\n",
      "batch 100 from 426 ...\n",
      "batch 150 from 426 ...\n",
      "batch 200 from 426 ...\n",
      "batch 250 from 426 ...\n",
      "batch 300 from 426 ...\n",
      "batch 350 from 426 ...\n",
      "batch 400 from 426 ...\n",
      "Epoch 30, Train Loss: 0.009275226223204905, Validation Loss: 0.6841282836187665\n"
     ]
    }
   ],
   "source": [
    "wandb.watch(model)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    batch_number = 0\n",
    "    for images, labels in train_loader:\n",
    "        if batch_number % 50 == 0:\n",
    "            print(f\"batch {batch_number} from {len(train_loader)} ...\")\n",
    "        batch_number += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)  # output shape: torch.Size([32, 29])\n",
    "        # the first dimension has a size of 32 due to our batch size (changes with different batch sizes)\n",
    "        # second dimension is 29 because we have 29 output labels\n",
    "\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            output = model(images)\n",
    "            loss = loss_function(output, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # step the scheduler - adjust the learning rate if validation loss stops decresing\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss/len(train_loader)}, Validation Loss: {val_loss/len(validation_loader)}\")\n",
    "    wandb.log({'epoch': epoch, 'training loss': train_loss, 'validation loss': val_loss, 'adjusted learning rate': optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    # Save model if validation loss has decreased\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        best_val_loss = val_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T15:33:01.941602100Z",
     "start_time": "2023-10-01T13:40:52.199595600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>adjusted learning rate</td><td>███████████████████████▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>training loss</td><td>█▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation loss</td><td>█▇▄▃▄▂▂▃▁▁▂▃▁▁▂▂▁▂▁▁▂▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>adjusted learning rate</td><td>0.001</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>training loss</td><td>3.95125</td></tr><tr><td>validation loss</td><td>73.20173</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">fresh-planet-8</strong> at: <a href='https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/mz69lif4' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-simpsons/runs/mz69lif4</a><br/> View job at <a href='https://wandb.ai/hamm-daniel/kaggle-simpsons/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMjc0MTY3MA==/version_details/v2' target=\"_blank\">https://wandb.ai/hamm-daniel/kaggle-simpsons/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMjc0MTY3MA==/version_details/v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20231001_154047-mz69lif4\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "  wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:06:21.180310600Z",
     "start_time": "2023-10-01T16:06:12.789765500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
