{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:08:21.221365900Z",
     "start_time": "2023-10-01T16:08:21.212340800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from shared_methods import all_labels, show_image_by_path\n",
    "from simpsons_neural_network_2 import SimpsonsNet2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check if we can use Cuda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# device = \"cpu\" # uncomment if you want to use \"cpu\", currently cpu is faster than cuda (maybe because the NN is very little)\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:08:22.332751200Z",
     "start_time": "2023-10-01T16:08:22.315758900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "SimpsonsNet2(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc1): Linear(in_features=25088, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=29, bias=True)\n  (dropout): Dropout(p=0.25, inplace=False)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = \"mz69lif4\"\n",
    "\n",
    "model = SimpsonsNet2()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(f\"trained_models/simpsons_net_2_{run_id}_4.pth\"))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:09:24.909759Z",
     "start_time": "2023-10-01T16:09:22.474760300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Create a DataLoader for the test dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:09:28.129549100Z",
     "start_time": "2023-10-01T16:09:28.118520Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# input_tensor = image_to_tensor(path)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m input_tensor \u001B[38;5;241m=\u001B[39m transform(Image\u001B[38;5;241m.\u001B[39mopen(path))\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 5\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Run inference\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# output tensor has shape [1, 29]\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# one batch dimension for my single image and 29 class scores\u001B[39;00m\n\u001B[0;32m     10\u001B[0m probabilities \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(output, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\dev\\projects\\kaggle-the-simpsons\\simpsons_neural_network_2.py:29\u001B[0m, in \u001B[0;36mSimpsonsNet2.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 29\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     30\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x))))\n\u001B[0;32m     31\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn3(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x))))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 138\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_input_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;66;03m# exponential_average_factor is set to self.momentum\u001B[39;00m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;66;03m# (when it is available) only so that it gets updated\u001B[39;00m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:410\u001B[0m, in \u001B[0;36mBatchNorm2d._check_input_dim\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_input_dim\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[1;32m--> 410\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected 4D input (got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mD input)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n",
      "\u001B[1;31mValueError\u001B[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    path = \"data/test/461179.jpg\"\n",
    "    # input_tensor = image_to_tensor(path)\n",
    "    input_tensor = transform(Image.open(path)).to(device)\n",
    "    output = model(input_tensor)  # Run inference\n",
    "\n",
    "    # output tensor has shape [1, 29]\n",
    "    # one batch dimension for my single image and 29 class scores\n",
    "\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    predicted_label_idx = torch.argmax(probabilities).item()\n",
    "\n",
    "    print(f\"I guess this is {all_labels[predicted_label_idx]}\")\n",
    "    show_image_by_path(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:09:33.745191900Z",
     "start_time": "2023-10-01T16:09:30.457840300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference method 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting batch 1 of 29\n",
      "Predicting batch 2 of 29\n",
      "Predicting batch 3 of 29\n",
      "Predicting batch 4 of 29\n",
      "Predicting batch 5 of 29\n",
      "Predicting batch 6 of 29\n",
      "Predicting batch 7 of 29\n",
      "Predicting batch 8 of 29\n",
      "Predicting batch 9 of 29\n",
      "Predicting batch 10 of 29\n",
      "Predicting batch 11 of 29\n",
      "Predicting batch 12 of 29\n",
      "Predicting batch 13 of 29\n",
      "Predicting batch 14 of 29\n",
      "Predicting batch 15 of 29\n",
      "Predicting batch 16 of 29\n",
      "Predicting batch 17 of 29\n",
      "Predicting batch 18 of 29\n",
      "Predicting batch 19 of 29\n",
      "Predicting batch 20 of 29\n",
      "Predicting batch 21 of 29\n",
      "Predicting batch 22 of 29\n",
      "Predicting batch 23 of 29\n",
      "Predicting batch 24 of 29\n",
      "Predicting batch 25 of 29\n",
      "Predicting batch 26 of 29\n",
      "Predicting batch 27 of 29\n",
      "Predicting batch 28 of 29\n",
      "Predicting batch 29 of 29\n",
      "Predicting batch 30 of 29\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"data/test\"\n",
    "batch_size = 32\n",
    "\n",
    "all_files = [f for f in os.listdir(root_dir) if f.endswith(\".jpg\")]\n",
    "num_batches = len(all_files) // batch_size\n",
    "\n",
    "with open(f\"solutions/solution_no1_{run_id}.csv\", \"w\") as output_file:\n",
    "    output_file.write(\"Id,Category\\n\")\n",
    "\n",
    "    # This loop iterates over the list of all test image files in batches. The step size is batch_size.\n",
    "    for i in range(0, len(all_files), batch_size):\n",
    "        print(f\"Predicting batch {i // batch_size + 1} of {num_batches}\")\n",
    "\n",
    "        # taking files from i to (i + batch_size)\n",
    "        batch_files = all_files[i:i + batch_size]\n",
    "        batch_tensors = [transform(Image.open(os.path.join(root_dir, f))) for f in batch_files]\n",
    "        batch_tensors = torch.stack(batch_tensors)\n",
    "\n",
    "        outputs = model(batch_tensors.to(device))  # shape ([32, 29]) (batch_size and number of classes)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)  # shape ([32, 29)]\n",
    "        predicted_indices = torch.argmax(probabilities, dim=1)  # shape ([32]) -> label for each element in the batch\n",
    "\n",
    "        for j, predicted_index in enumerate(predicted_indices):\n",
    "            output_file.write(f\"{batch_files[j]},{all_labels[predicted_index.item()]}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:10:03.269274200Z",
     "start_time": "2023-10-01T16:09:59.357869400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference method 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting file with index 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root_dir, filename)\n\u001B[0;32m     15\u001B[0m input_tensor \u001B[38;5;241m=\u001B[39m transform(Image\u001B[38;5;241m.\u001B[39mopen(img_path))\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 16\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m probabilities \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(output, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     19\u001B[0m predicted_label_idx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(probabilities)\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\dev\\projects\\kaggle-the-simpsons\\simpsons_neural_network_2.py:29\u001B[0m, in \u001B[0;36mSimpsonsNet2.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 29\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     30\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x))))\n\u001B[0;32m     31\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn3(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x))))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 138\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_input_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;66;03m# exponential_average_factor is set to self.momentum\u001B[39;00m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;66;03m# (when it is available) only so that it gets updated\u001B[39;00m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\kaggle-the-simpsons-env\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:410\u001B[0m, in \u001B[0;36mBatchNorm2d._check_input_dim\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_input_dim\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[1;32m--> 410\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected 4D input (got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mD input)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n",
      "\u001B[1;31mValueError\u001B[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "root_dir = \"data/test\"\n",
    "\n",
    "output_file = open(f\"solutions/solution_no2_{run_id}.csv\", \"w\")\n",
    "output_file.write(\"Id,Category\\n\")\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for filename in os.listdir(root_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Predicting file with index {idx}\")\n",
    "        idx += 1\n",
    "\n",
    "        img_path = os.path.join(root_dir, filename)\n",
    "        input_tensor = transform(Image.open(img_path)).to(device)\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_label_idx = torch.argmax(probabilities).item()\n",
    "\n",
    "        output_file.write(f\"{filename},{all_labels[predicted_label_idx]}\\n\")\n",
    "\n",
    "output_file.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T16:10:08.162522100Z",
     "start_time": "2023-10-01T16:10:08.078524800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
